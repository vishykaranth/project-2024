# Alerting: Alert Rules, Notification Channels, Escalation

## Overview

Alerting is the process of automatically detecting anomalies, threshold violations, or critical conditions in a system and notifying relevant stakeholders. Effective alerting ensures that teams are informed about issues promptly, enabling quick response and resolution.

## Alerting Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Alerting System Architecture               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Metrics/Logs/Traces
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Monitoring  â”‚  â† Collects data
â”‚  System      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ (Evaluates)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Alert Rules â”‚  â† Defines conditions
â”‚  Engine      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ (Triggers)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Alert       â”‚  â† Manages alert state
â”‚  Manager     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ (Routes)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Notification â”‚  â† Sends notifications
â”‚  Channels    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ (Delivers)
       â”‚
    â”Œâ”€â”€â”´â”€â”€â”
    â”‚     â”‚
    â–¼     â–¼
â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”
â”‚Emailâ”‚ â”‚Slackâ”‚ â”‚PagerDutyâ”‚ â”‚SMSâ”‚
â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”˜
```

## Alert Rules

### Alert Rule Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Alert Rule Components                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Alert Rule:
â”œâ”€ Name: Descriptive alert name
â”œâ”€ Condition: When to trigger
â”œâ”€ Threshold: Value that triggers alert
â”œâ”€ Duration: How long condition must persist
â”œâ”€ Severity: Critical, Warning, Info
â”œâ”€ Labels: Metadata for routing
â””â”€ Annotations: Human-readable information
```

### Prometheus Alert Rules

**Example Alert Rule:**

```yaml
# alerts.yml
groups:
  - name: api_errors
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec (threshold: 0.05)"
      
      - alert: HighLatency
        expr: histogram_quantile(0.95, 
              rate(http_request_duration_seconds_bucket[5m])) > 1.0
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }}s (threshold: 1.0s)"
      
      - alert: ServiceDown
        expr: up{job="api-service"} == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Service is down"
          description: "{{ $labels.instance }} is down"
```

### Alert Rule Types

**1. Threshold Alerts**
```yaml
# CPU usage above 80%
- alert: HighCPUUsage
  expr: cpu_usage_percent > 80
  for: 5m
```

**2. Rate Alerts**
```yaml
# Error rate increasing
- alert: IncreasingErrorRate
  expr: rate(http_errors_total[5m]) > 0.1
  for: 5m
```

**3. Absence Alerts**
```yaml
# No metrics received
- alert: NoMetricsReceived
  expr: absent(up{job="api-service"})
  for: 5m
```

**4. Anomaly Alerts**
```yaml
# Unusual pattern
- alert: AnomalousTraffic
  expr: http_requests_total > (avg_over_time(http_requests_total[1h]) * 2)
  for: 10m
```

## Alert States

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Alert State Machine                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Inactive
    â”‚
    â”‚ (Condition met)
    â”‚
    â–¼
Pending
    â”‚
    â”‚ (Duration elapsed)
    â”‚
    â–¼
Firing
    â”‚
    â”‚ (Condition no longer met)
    â”‚
    â–¼
Resolved
    â”‚
    â”‚ (Back to normal)
    â”‚
    â””â”€â”€â”€â–º Inactive
```

### Alert States Explained

**1. Inactive**
- Condition not met
- No alert triggered
- Normal state

**2. Pending**
- Condition met
- Waiting for duration
- Not yet firing

**3. Firing**
- Condition met for duration
- Alert active
- Notifications sent

**4. Resolved**
- Condition no longer met
- Alert cleared
- Resolution notification sent

## Notification Channels

### Channel Types

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Notification Channels                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”œâ”€ Email
â”‚  â”œâ”€ Pros: Universal, detailed
â”‚  â””â”€ Cons: Can be ignored, delayed
â”‚
â”œâ”€ Slack/Teams
â”‚  â”œâ”€ Pros: Real-time, team visibility
â”‚  â””â”€ Cons: Can be noisy
â”‚
â”œâ”€ PagerDuty/Opsgenie
â”‚  â”œâ”€ Pros: On-call management, escalation
â”‚  â””â”€ Cons: Cost, complexity
â”‚
â”œâ”€ SMS
â”‚  â”œâ”€ Pros: Immediate, reliable
â”‚  â””â”€ Cons: Cost, limited content
â”‚
â”œâ”€ Webhooks
â”‚  â”œâ”€ Pros: Flexible, custom integrations
â”‚  â””â”€ Cons: Requires development
â”‚
â””â”€ Phone Calls
   â”œâ”€ Pros: Guaranteed delivery
   â””â”€ Cons: Intrusive, expensive
```

### Alertmanager Configuration

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/...'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
      continue: true
    
    - match:
        severity: warning
      receiver: 'warning-alerts'

receivers:
  - name: 'default'
    email_configs:
      - to: 'team@example.com'
        headers:
          Subject: 'Alert: {{ .GroupLabels.alertname }}'
  
  - name: 'critical-alerts'
    pagerduty_configs:
      - service_key: 'xxx'
        description: '{{ .GroupLabels.alertname }}'
    
    slack_configs:
      - channel: '#alerts-critical'
        title: 'ðŸš¨ Critical Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
  
  - name: 'warning-alerts'
    slack_configs:
      - channel: '#alerts-warning'
        title: 'âš ï¸ Warning'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
```

## Alert Routing

### Routing Rules

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Alert Routing Logic                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Alert Fired
    â”‚
    â–¼
Check Labels
    â”‚
    â”œâ”€â–º severity: critical
    â”‚   â””â”€â–º Route to PagerDuty + Slack
    â”‚
    â”œâ”€â–º severity: warning
    â”‚   â””â”€â–º Route to Slack
    â”‚
    â”œâ”€â–º team: backend
    â”‚   â””â”€â–º Route to Backend Team
    â”‚
    â””â”€â–º team: infrastructure
        â””â”€â–º Route to DevOps Team
```

### Routing Configuration

```yaml
route:
  routes:
    # Critical alerts go to on-call
    - match:
        severity: critical
      receiver: on-call
      continue: false
    
    # Backend team alerts
    - match:
        team: backend
      receiver: backend-team
    
    # Infrastructure alerts
    - match:
        team: infrastructure
      receiver: devops-team
    
    # Default route
    - receiver: default
```

## Alert Grouping

### Grouping Benefits

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Alert Grouping                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Without Grouping:
â”œâ”€ Alert: HighCPU on server1
â”œâ”€ Alert: HighCPU on server2
â”œâ”€ Alert: HighCPU on server3
â””â”€ Alert: HighCPU on server4
   â†’ 4 separate notifications

With Grouping:
â””â”€ Alert: HighCPU (4 instances)
   â”œâ”€ server1
   â”œâ”€ server2
   â”œâ”€ server3
   â””â”€ server4
   â†’ 1 grouped notification
```

### Grouping Configuration

```yaml
route:
  group_by: ['alertname', 'cluster']
  group_wait: 10s      # Wait before sending
  group_interval: 5m    # Wait between groups
  repeat_interval: 12h  # Repeat if still firing
```

## Escalation Policies

### Escalation Chain

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Escalation Policy                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Level 1: On-Call Engineer (0-15 min)
    â”‚
    â”‚ (No acknowledgment)
    â”‚
    â–¼
Level 2: Team Lead (15-30 min)
    â”‚
    â”‚ (No acknowledgment)
    â”‚
    â–¼
Level 3: Engineering Manager (30-60 min)
    â”‚
    â”‚ (No acknowledgment)
    â”‚
    â–¼
Level 4: CTO (60+ min)
```

### PagerDuty Escalation

```json
{
  "escalation_policy": {
    "name": "Production Escalation",
    "escalation_rules": [
      {
        "escalation_delay_in_minutes": 0,
        "targets": [
          {
            "type": "user",
            "id": "on-call-engineer-id"
          }
        ]
      },
      {
        "escalation_delay_in_minutes": 15,
        "targets": [
          {
            "type": "user",
            "id": "team-lead-id"
          }
        ]
      },
      {
        "escalation_delay_in_minutes": 30,
        "targets": [
          {
            "type": "user",
            "id": "engineering-manager-id"
          }
        ]
      }
    ]
  }
}
```

## Alert Fatigue Prevention

### Strategies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Preventing Alert Fatigue                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”œâ”€ Set Appropriate Thresholds
â”‚  â””â”€ Don't alert on every minor issue
â”‚
â”œâ”€ Use Alert Severity
â”‚  â”œâ”€ Critical: Immediate action needed
â”‚  â”œâ”€ Warning: Attention needed
â”‚  â””â”€ Info: Informational only
â”‚
â”œâ”€ Implement Alert Suppression
â”‚  â””â”€ Suppress during maintenance
â”‚
â”œâ”€ Group Related Alerts
â”‚  â””â”€ Reduce notification volume
â”‚
â”œâ”€ Use Alert Routing
â”‚  â””â”€ Route to right people
â”‚
â””â”€ Regular Alert Review
   â””â”€ Remove unnecessary alerts
```

### Alert Suppression

```yaml
# Suppress alerts during maintenance
inhibit_rules:
  - source_match:
      severity: 'maintenance'
    target_match:
      severity: 'critical'
    equal: ['alertname', 'instance']
```

## Best Practices

### 1. Alert Naming
- Use descriptive names
- Include context
- Example: "HighErrorRate-API-Service"

### 2. Thresholds
- Set based on SLOs
- Use percentiles (p95, p99)
- Consider business impact

### 3. Duration
- Avoid instant alerts
- Use appropriate wait times
- Prevent false positives

### 4. Runbooks
- Document alert responses
- Include troubleshooting steps
- Link from alert annotations

### 5. Testing
- Test alert rules
- Verify notifications
- Validate escalation

## Alert Examples

### Infrastructure Alerts

```yaml
# High CPU usage
- alert: HighCPUUsage
  expr: cpu_usage_percent > 80
  for: 5m
  annotations:
    summary: "High CPU usage on {{ $labels.instance }}"
    runbook_url: "https://wiki/runbooks/high-cpu"

# Disk space low
- alert: LowDiskSpace
  expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1
  for: 10m
  annotations:
    summary: "Disk space below 10% on {{ $labels.instance }}"
```

### Application Alerts

```yaml
# High error rate
- alert: HighErrorRate
  expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
  for: 5m
  annotations:
    summary: "Error rate above 5% for {{ $labels.service }}"

# High latency
- alert: HighLatency
  expr: histogram_quantile(0.95, 
        rate(http_request_duration_seconds_bucket[5m])) > 1.0
  for: 10m
  annotations:
    summary: "95th percentile latency above 1s"
```

### Business Alerts

```yaml
# Low transaction volume
- alert: LowTransactionVolume
  expr: rate(transactions_total[1h]) < 100
  for: 30m
  annotations:
    summary: "Transaction volume below expected threshold"
```

## Summary

Alerting:
- **Purpose**: Detect and notify about issues automatically
- **Components**: Alert rules, notification channels, escalation
- **States**: Inactive â†’ Pending â†’ Firing â†’ Resolved
- **Tools**: Prometheus Alertmanager, PagerDuty, Opsgenie

**Key Concepts:**
- Alert Rules: Define when to alert
- Notification Channels: How to notify
- Escalation: When to escalate
- Grouping: Reduce noise
- Suppression: Prevent fatigue

**Best Practices:**
- Set appropriate thresholds
- Use severity levels
- Implement escalation
- Group related alerts
- Regular review and cleanup
